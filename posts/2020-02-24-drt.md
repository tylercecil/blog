---
title: Donkey Sentences
subtitle: The Limits of Lexical Scope
abstract: A very brief introduction to *Discourse Representation Theory*, a
  method of using dynamic scope to better model natural language. Featuring
  Russell and *his* donkey.
update:
note:
references:
- id: stan-drt
  type: chapter
  author:
  - family: Geurts
    given: Beaver
    suffix: Bart
  - family: Maier
    given: Emar
  editor:
  - family: Zalta
    given: Edward N.
  issued:
  - year: 2020
  title: Discourse representation theory
  container-title: The stanford encyclopedia of philosophy
  publisher: <https://plato.stanford.edu/archives/spr2020/entries/discourse-representation-theory/>;
    Metaphysics Research Lab, Stanford University
  edition: Spring 2020
- id: kamp
  type: chapter
  author:
  - family: Kamp
    given: Hans
  - family: Genabith
    given: Josef
  - family: Reyle
    given: Uwe
  issued:
  - year: 2010
    month: 11
  title: Discourse representation theory
  container-title: Handbook of Philosophical Logic, Handbook of Philosophical Logic,
    Volume 15. ISBN 978-94-007-0484-8. Springer Science+Business Media B.V., 2011,
    p. 125
  page: 125-394
  DOI: 10.1007/978-94-007-0485-5_3
  URL: https://www.researchgate.net/publication/226622246_Discourse_Representation_Theory
- id: partee
  type: article-journal
  author:
  - family: Partee
    given: Barbara H.
  issued:
  - year: 1984
  title: Nominal and temporal anaphora
  container-title: Linguistics and Philosophy
  publisher: Springer
  page: 243-286
  volume: '7'
  issue: '3'
  URL: http://www.jstor.org/stable/25001168
  ISSN: 01650157, 15730549
- id: kamp81
  type: article-journal
  author:
  - family: Kamp
    given: Hans
  issued:
  - year: 1981
  title: A theory of truth and semantic representation
  container-title: Formal semantics-the essential readings
  page: 189-222

header-includes: >
  <style>
    .n1 {
      color: green;
    }

    .n2 {
      color: blue;
    }

    .nq {
      color: red;
    }

    .n1::after {
      font-size: 75%;
      bottom: -0.25em;
      position: relative;
      content: "1";
    }

    .n2::after {
      font-size: 75%;
      bottom: -0.25em;
      position: relative;
      content: "2";
    }

    .nq::after {
      font-size: 75%;
      bottom: -0.25em;
      position: relative;
      content: "?";
    }
  </style>
---

<div style="display:none;">
$\newcommand{drs}[2]{
  \begin{array}{|c|}
    \hline
    #1 \\\hline
    #2 \\\hline
  \end{array}}$
</div>

In semantics, we typically take a sentence, and convert it into a formal
logical representation.[^mont] *"All dogs bark"* might be represented as
*$[\forall x$ : DOG$(x)]$ BARK$(x)$.* Formal logical systems tend to use
**lexical scope** --- "$\forall$" introduces an $x$, some things are said about
$x$, and then we throw the $x$ away.

My background is in PL Theory, so this makes me feel pretty warm and fuzzy
inside. But... this might be a case of trying to fit a round peg into a square
hole, as can be demonstrated by a man and his donkey:

(@smile) [Russell]{.n1} saw a [donkey]{.n2}. [He]{.n1} smiled at [it]{.n2}.

We could be lazy, and just say [he]{.n1} and [it]{.n2} come from "context", and
leave it at that. But even then, what does this look like in a logical
representation?

$$
\begin{align*}
  [\exists x : \text{DONKEY}(x) ]\; &\text{SEE}(r, x)\\
  [\exists x : \text{DONKEY}(x) ]\; &\text{SMILE}(r, x)\\
\end{align*}
$$

That obviously doesn't work, because the donkey Russell saw was the same one
he smiled at. Maybe we could "merge" these sentences together?

$$
[\exists x : \text{DONKEY}(x) ]\; \text{SEE}(r, x) \wedge \text{SMILE}(r, x)
$$

But why did the scope of "$\exists$" extend to the next sentence? What if we
put a sentence between the two? And we still haven't dealt with [he]{.n1} and
[it]{.n2}!

This week I want to dive into so-called *Donkey Sentences*, and the advantages
of tossing lexical-scope out the window, using a technique known as **Discourse
Representation Theory**, or **DRT**.

[^mont]: Specifically, this is the tradition of Montague grammar (which is
  pretty pervasive). Montague's thesis is essentially that *natural languages*
  and *formal languages* (like programming languages) ultimately can be treated
  the same way.

Anaphora
========

> *Did you just say "Donkey Sentences?"*

Why yes I did --- [they're a
thing](https://en.wikipedia.org/wiki/Donkey_sentence)! Donkey sentences are
totally correct sentences, that have clear meanings, but have one small
problem: they use *anaphora*.

Anaphora
: Any expression whose interpretation depends on another expression elsewhere
  (called the **antecedent** if it occurred before, **postcedent** if it
  occurred after).\
  e.g. "[The president was there]{.n1}, and [**that**]{.n1} made everyone
  uncomfortable." The meaning of [that]{.n1} depends on the antecedent
  [the president was there]{.n1}.

So in (@smile) the anaphora were [he]{.n1} and [it]{.n2}.

When I say *"depends on"*, what do I mean exactly? At first glance, you might
think you can simply cut-and-past anaphora out of a sentence.

(@) [Russell]{.n1} woke up. [He]{.n1} was surprised to see [his]{.n1} donkey.
(@cut) *Russell* was surprised to see *Russell's* donkey.

That would make our life easy, but the simple solution of (@cut) doesn't always
work. Trying the same method on (@smile):

(@bsmile) Russel saw a donkey. *Russell* smiled at *a donkey*.

This isn't exactly the same sentence --- it's not clear that we're talking
about the same donkey.[^elab] It's also worth mentioning that anaphora don't
just extend across sentences, but can happen within sentences, too!

(@if) If [Russell]{.n1} had [a donkey]{.n2}, [he]{.n1} would beat [it]{.n2}.
(@bcut) If Russell had a donkey, *Russell* would beat *a donkey*.

Yeah, that's just no good.

An interesting note here, is cutting-and-pasting the word "Russell" seems to be
just fine; it's this "a donkey" that's causing problems. Let's keep this in the
back of our minds for now.

[^elab]: This is a bit more clear with a sentence added in the middle. e.g.
  *"Russel saw a donkey. In fact, he saw hundreds of donkeys. Russel smiled at
  a donkey".*

Not just Nouns!
---------------

It's worth pointing out there there are *all kinds* of anaphora, not just
pronouns. These (and others) all occur cross-linguistically.[^sign]

[^sign]: This includes sign languages, which I find to be objectively cool.
  Look up some recent work by Philippe Schlenker for more information.


Table: For more information on anaphora, I'd look at
  [The Stanford Encyclopedia of Philosophy](https://plato.stanford.edu/entries/anaphora).

-----------------  ------------------------------------------------------------
**Verb Phrase**    "I just got a donkey. Irma did too." (Here *"did"* refers
                   back to last sentence)

**Propositional**  "Russell says it was his donkey. I don't believe that."
                   (What is it I don't beleive?)

**Adjectival**     This was a stubborn donkey. I guess all donkeys are. (We
                   just dropped the last word of the sentence, "stubborn"!)

**Modal**          Russell might buy a donkey. He would pay cash. (The world in
                   which Russell is paying cash is the same world where he is
                   buying a donkey.)

**Temporal**       Russell had a party last Friday, and his donkey got drunk.
                   (The time in which the donkey got drunk was during the
                   party.)
-------------------------------------------------------------------------------

Scope Woes
==========

So why was it that "Russell" could be copied-and-pasted in sentences like
(@bsmile) and (@bcut), but "a donkey" couldn't? Looking back at the logical
representation, we have

$$
[\exists x : \text{DONKEY}(x) ]\; \text{SEE}(r, x)
$$

We converted "a donkey" into $[\exists x : \text{DONKEY}(x) ]$, which is
pretty reasonable. However, this commits us to a scope for $x$ --- outside this
$\exists$-expression, $x$ will be thrown out.

Russell, on the other hand, was simply assigned to $r$. $r$ has no quantifier,
and therefore no scope. Russel is "global" --- there is only one (relevant)
Russell to speak of, and we can put him wherever we like in our sentences.

And that's the main difference: Russell isn't giving us any trouble, because
$r$ has no scope. "A donkey", however, is confusing because **it's not clear
how big the scope of $x$ should be.**

Dynamic Scoping
---------------

Here's a hot take: **language is stateful.** When I say something about a
donkey, you don't just throw it away once the sentence is over. Maybe this
donkey will make an appearance later on down the road --- who's to say? You
incrementally keep a model of what is being said. This is so intuitive, it
borders on obvious, yet trying to fit language into lexical scope is wilfully
ignoring this reality!

This was the intuition of @kamp81, and it caused him to develop **Discourse
Representation Theory**. At its core, *DRT* and systems like it involve keeping
track of a **dynamic scope**.[^bash]

Let's see what this actually looks like.

[^bash]: If you want a programming example to draw intuition from, I would
  point out `bash`. In `bash`, all variables are global, and they only come
  into existence when you first assign something to them.

DRT
===

*DRT* revolves around the idea of a *Discourse Representation Structure*.

Discourse Representation Structure (DRS)
: A structure made up of two lists:

    - A list of *discourse referents* (the things being talked about)
    - A list of the *conditions* (what is being said about them)[^drt]

[^drt]: This is actually essentially the same of converting to logical
  notation, but now we keep a dynamic list of names.

As sentences are received, the *DRT* becomes updated. Let's start with the
first half of our original sentence.

(@d1) Russell saw a donkey.

We started with an empty DRS, which just became updated after (@d1).

$$
\drs{r, x}{\text{RUSSELL}(r)\\ \text{DONKEY}(x) \\ \text{SEE}(r, x)}
$$

With this state in our brains, we then receive the second sentence.

(@) He smiled at it.

Now we have two new names, "he" and "it" (both of which are anaphora), and one
new conditional.

$$
\drs{r, x}{\text{RUSSELL}(r)\\ \text{DONKEY}(x) \\ \text{SEE}(r, x)}
\Rightarrow
\drs{r, x, \hat{y}, \hat{z}}
{\text{RUSSELL}(r)\\ \text{DONKEY}(x) \\ \text{SEE}(r, x) \\
\text{SMILE}(\hat{y}, \hat{z})}
$$

The anaphora $\hat{y}$ and $\hat{z}$ at this point are incomplete --- we need
to resolve which antecedent goes with which. This is actually trivial in our
example. $\hat{y}$ can *only* map to $r$, and $\hat{z}$ to $x$.[^syn]

[^syn]: This is because the syntax of "he" requires a male.

$$
\drs{r, x, \hat{y}, \hat{z}}
{\text{RUSSELL}(r)\\ \text{DONKEY}(x) \\ \text{SEE}(r, x) \\
\text{SMILE}(\hat{y}, \hat{z})}
\Rightarrow
\drs{r, x, \hat{y}, \hat{z}}
{r = \hat{y} \\ x = \hat{z} \\
\text{RUSSELL}(r)\\ \text{DONKEY}(x) \\ \text{SEE}(r, x) \\
\text{SMILE}(\hat{y}, \hat{z})}
\Rightarrow
\drs{r, x}
{\text{RUSSELL}(r)\\ \text{DONKEY}(x) \\ \text{SEE}(r, x) \\
\text{SMILE}(r, x)}
$$

Some Intuition on New Names
---------------------------

So much of this work revolves around rethinking what quantifiers mean in a
dynamic context. Should we be adding discourse referents for every noun we see?

Probably not. Consider if I said:

(@add) Russell loves a donkey. The donkey does not love him back.

When you hear "Russell", your brain says "ok, we're talking about a guy named
Russell". Then you hear "a donkey" --- "ah! A donkey has entered the picture".
"The donkey" however, did not add a new character to our world. Because it is
definite[^def], we know not to add a new referent. Any time we hear the
indefinite "a farmer" or "another donkey", however, we know to add a discourse
referent.

[^def]: Definite here means "there is only one". By saying "the $x$", you are
  stating that there is only one $x$. Pronouns are another form of definite
  references.

Negation of DRSs
----------------

It turns out global scope isn't exactly always what you want.

(@neg) #There is [no donkey]{.n1}. Russell smiles at [it]{.nq}.

[It]{.nq} here certainly can't refer to [no donkey]{.n1}! This
negative-quantifier has a very narrow scope --- you can never refer back to the
donkey that doesn't exist. To deal with this, we can nest DRSs!

$$
\drs{r, \hat{y}}{
\text{RUSSELL}(r) \\ \text{SMILE}(r, \hat{y}) \\
\neg \; \drs{x}{\text{DONKEY}(x)}_{R_1}
}_{R_0}
$$

We can read this as **"any extension to $R_0$ which includes the content of
$R_1$ must be false".**

Conditionals
------------

So how do we model conditionals with this system?

(@if) If [Russell]{.n1} had [a donkey]{.n2}, [he]{.n1} would beat [it]{.n2}.

Again, we're going to use a method of nested DRSs.

$$
\drs{r}{ \text{RUSSELL}(r) \\
\drs{x}{ \text{DONKEY}(x) \\ \text{HAS}(r,x) }_{R_1} \Rightarrow
\drs{\hat{y}, \hat{z}}{ \text{BEAT}(\hat{y}, \hat{z}) }_{R_2}
}_{R_0}
$$

A lot like negation, $R_1 \Rightarrow R_2$ is giving us rules about how $R_0$
might be expanded. Namely, **any extension which includes the contents of $R_1$
must also include the contents of $R_2$.**

Dynamic is Useful
=================

Obviously I've only barely introduced DRT[^poll], but it might quickly be
apparent how *dynamic semantics* can be very powerful. It seems to address
anaphora fairly well, but that's really just the beginning.

DRT is also something that can be easily modeled by a computer. This has led to
a whole swath of fruitful research in AI and Computational Linguistics. For the
same reason, it also bridges formal semantics with psychological models of
language.

What I find particularly interesting, is that dynamic semantics also seems to
bring the *pragmatic-semantic* interface more into focus. **Pragmatics**, which
is loosely the subfield of linguistics dealing with how context contributes to
meaning, has been a rather elusive topic. With a DRS, however, we can more
clearly see when and how pragmatics jumps into the picture --- for example,
searching for an antecedent $x$ to bind to some anaphora $\hat{y}$.

[^poll]: **Question for my readers:** have my posts been too "introductory"?
  Would a deep-dive into topics be more interesting than a brief overview?
  Because I haven't been writing much about novel work, I've been under the
  assumption that those who wish to learn more will read the source research.
  There are a number of extensions to DRT, critiques, and we haven't touched at
  all on judgments within the system, or computational implementations.

Parting Notes
=============

If you'd like a deeper look at DRT, I'd strongly suggest @kamp and @stan-drt,
probably in that order. They both read very easy, and go much more in-depth on
the subject.

I haven't seen the actual phrase "dynamic scope" used in discussion about these
theories, but the idea is there. I'm just using my computer science vocabulary.

I actually originally wanted to write exclusively about @partee, and her use
of DRT for the analysis of tense, but... I couldn't actually find a copy of the
paper online! The tragic realities of not being affiliated with a university...
I hope to post a mini-post soon, introducing those ideas.

If I were to write a part-two, I think I'd like to look at judgments,
inferences, and maybe even a computational implementation (this feel like it
would be fun to implement in Coq, for example). It's also worth noting that
there are a number of different *flavors* of DRT, each with their own pros and
cons.

Stay tuned later this week for a spiritual sequel, though! I have a fun topic
planed related to anaphora.

**Thanks as always for reading!** Every week I write about something I'm
learning to test my own understanding, but also to work on my writing and
explanatory skills. If you have questions, critiques, or thoughts, please
**leave a comment!** They might teach me something, and they'll let me know
someone's reading!

References
==========
:::refs
:::
