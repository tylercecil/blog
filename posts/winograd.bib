@article{levesque-2014,
	title = {On our best behaviour},
	volume = {212},
	issn = {0004-3702},
	url = {http://www.sciencedirect.com/science/article/pii/S0004370214000356},
	doi = {10.1016/j.artint.2014.03.007},
	abstract = {The science of {AI} is concerned with the study of intelligent forms of behaviour in computational terms. But what does it tell us when a good semblance of a behaviour can be achieved using cheap tricks that seem to have little to do with what we intuitively imagine intelligence to be? Are these intuitions wrong, and is intelligence really just a bag of tricks? Or are the philosophers right, and is a behavioural understanding of intelligence simply too weak? I think both of these are wrong. I suggest in the context of question-answering that what matters when it comes to the science of {AI} is not a good semblance of intelligent behaviour at all, but the behaviour itself, what it depends on, and how it can be achieved. I go on to discuss two major hurdles that I believe will need to be cleared.},
	pages = {27--35},
	journaltitle = {Artificial Intelligence},
	shortjournal = {Artificial Intelligence},
	author = {Levesque, Hector J.},
	urldate = {2020-02-27},
	date = {2014-07-01},
	langid = {english},
	keywords = {winograd},
	file = {ScienceDirect Full Text PDF:/home/tyler/Zotero/storage/XYV64EH3/Levesque - 2014 - On our best behaviour.pdf:application/pdf;ScienceDirect Snapshot:/home/tyler/Zotero/storage/K4QQUIVC/S0004370214000356.html:text/html}
}

@article{winograd-1972,
	title = {Understanding natural language},
	volume = {3},
	issn = {0010-0285},
	url = {http://www.sciencedirect.com/science/article/pii/0010028572900023},
	doi = {10.1016/0010-0285(72)90002-3},
	abstract = {This paper describes a computer system for understanding English. The system answers questions, executes commands, and accepts information in an interactive English dialog. It is based on the belief that in modeling language understanding, we must deal in an integrated way with all of the aspects of languageâ€”syntax, semantics, and inference. The system contains a parser, a recognition grammar of English, programs for semantic analysis, and a general problem solving system. We assume that a computer cannot deal reasonably with language unless it can understand the subject it is discussing. Therefore, the program is given a detailed model of a particular domain. In addition, the system has a simple model of its own mentality. It can remember and discuss its plans and actions as well as carrying them out. It enters into a dialog with a person, responding to English sentences with actions and English replies, asking for clarification when its heuristic programs cannot understand a sentence through the use of syntactic, semantic, contextual, and physical knowledge. Knowledge in the system is represented in the form of procedures, rather than tables of rules or lists of patterns. By developing special procedural representations for syntax, semantics, and inference, we gain flexibility and power. Since each piece of knowledge can be a procedure, it can call directly on any other piece of knowledge in the system.},
	pages = {1--191},
	number = {1},
	journaltitle = {Cognitive Psychology},
	shortjournal = {Cognitive Psychology},
	author = {Winograd, Terry},
	urldate = {2020-02-27},
	date = {1972-01-01},
	langid = {english},
	keywords = {find\_me, winograd},
	file = {ScienceDirect Snapshot:/home/tyler/Zotero/storage/CIZFZBC5/0010028572900023.html:text/html}
}

@article{liu-2016,
	title = {Commonsense Knowledge Enhanced Embeddings for Solving Pronoun Disambiguation Problems in Winograd Schema Challenge},
	url = {http://arxiv.org/abs/1611.04146},
	abstract = {In this paper, we propose commonsense knowledge enhanced embeddings ({KEE}) for solving the Pronoun Disambiguation Problems ({PDP}). The {PDP} task we investigate in this paper is a complex coreference resolution task which requires the utilization of commonsense knowledge. This task is a standard first round test set in the 2016 Winograd Schema Challenge. In this task, traditional linguistic features that are useful for coreference resolution, e.g. context and gender information, are no longer effective anymore. Therefore, the {KEE} models are proposed to provide a general framework to make use of commonsense knowledge for solving the {PDP} problems. Since the {PDP} task doesn't have training data, the {KEE} models would be used during the unsupervised feature extraction process. To evaluate the effectiveness of the {KEE} models, we propose to incorporate various commonsense knowledge bases, including {ConceptNet}, {WordNet}, and {CauseCom}, into the {KEE} training process. We achieved the best performance by applying the proposed methods to the 2016 Winograd Schema Challenge. In addition, experiments conducted on the standard {PDP} task indicate that, the proposed {KEE} models could solve the {PDP} problems by achieving 66.7\% accuracy, which is a new state-of-the-art performance.},
	journaltitle = {{arXiv}:1611.04146 [cs]},
	author = {Liu, Quan and Jiang, Hui and Ling, Zhen-Hua and Zhu, Xiaodan and Wei, Si and Hu, Yu},
	urldate = {2020-02-27},
	date = {2016-12-21},
	eprinttype = {arxiv},
	eprint = {1611.04146},
	keywords = {winograd},
	file = {arXiv.org Snapshot:/home/tyler/Zotero/storage/9X5BVPYI/1611.html:text/html;Liu et al. - 2016 - Commonsense Knowledge Enhanced Embeddings for Solv.pdf:/home/tyler/Zotero/storage/2H828WFT/Liu et al. - 2016 - Commonsense Knowledge Enhanced Embeddings for Solv.pdf:application/pdf}
}

@online{richard-bollans-2018,
	title = {The Role of Pragmatics in Solving the Winograd Schema Challenge},
	url = {http://ceur-ws.org/Vol-2052/},
	abstract = {Different aspects and approaches to commonsense reasoning have been investigated in order to provide solutions for the Winograd Schema Challenge ({WSC}). The vast complexities of natural language processing (parsing, assigning word sense, integrating context, pragmatics and world-knowledge, ...) give broad appeal to systems based on statistical analysis of corpora. However, solutions based purely on learning from corpora are not currently able to capture the semantics underlying the {WSC} - which was intended to provide problems whose solution requires knowledge and reasoning, rather than statistical analysis of superficial lexical features. In this paper we consider the {WSC} as a means for highlighting challenges in the field of commonsense reasoning more generally. We begin by discussing issues with current approaches to the {WSC}. Following this we outline some key challenges faced, in particular highlighting the importance of dealing with pragmatics. We then argue for an alternative approach which favours the use of knowledge bases where the deep semantics of the different interpretations of commonsense terms are formalised. Furthermore, we suggest using heuristic approaches based on pragmatics to determine appropriate configurations of both reasonable interpretations of terms and necessary assumptions about the world.},
	titleaddon = {Proceedings of the Thirteenth International Symposium on Commonsense Reasoning (Commonsense 2017)},
	type = {Proceedings Paper},
	author = {Richard-Bollans, A. L. and Gomez Alvarez, L. and Cohn, A. G.},
	urldate = {2020-02-27},
	date = {2018-01-23},
	langid = {english},
	keywords = {winograd},
	file = {Full Text PDF:/home/tyler/Zotero/storage/WGBTK3M2/Richard-Bollans et al. - 2018 - The Role of Pragmatics in Solving the Winograd Sch.pdf:application/pdf;Snapshot:/home/tyler/Zotero/storage/JQID3J8F/122937.html:text/html}
}

@inproceedings{levesque-2012,
	title = {The Winograd Schema Challenge},
	booktitle = {Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning},
	author = {Levesque, Hector and Davis, Ernest and Morgenstern, Leora},
	date = {2012},
	keywords = {winograd},
	file = {Full Text:/home/tyler/Zotero/storage/UUHFKN55/Levesque et al. - The Winograd Schema Challenge.pdf:application/pdf;Snapshot:/home/tyler/Zotero/storage/8LNCS8CG/4492.html:text/html}
}

@article{kocijan-2019,
	title = {A Surprisingly Robust Trick for Winograd Schema Challenge},
	url = {http://arxiv.org/abs/1905.06290},
	doi = {10.18653/v1/P19-1478},
	abstract = {The Winograd Schema Challenge ({WSC}) dataset {WSC}273 and its inference counterpart {WNLI} are popular benchmarks for natural language understanding and commonsense reasoning. In this paper, we show that the performance of three language models on {WSC}273 strongly improves when fine-tuned on a similar pronoun disambiguation problem dataset (denoted {WSCR}). We additionally generate a large unsupervised {WSC}-like dataset. By fine-tuning the {BERT} language model both on the introduced and on the {WSCR} dataset, we achieve overall accuracies of 72.5\% and 74.7\% on {WSC}273 and {WNLI}, improving the previous state-of-the-art solutions by 8.8\% and 9.6\%, respectively. Furthermore, our fine-tuned models are also consistently more robust on the "complex" subsets of {WSC}273, introduced by Trichelair et al. (2018).},
	pages = {4837--4842},
	journaltitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
	author = {Kocijan, Vid and Cretu, Ana-Maria and Camburu, Oana-Maria and Yordanov, Yordan and Lukasiewicz, Thomas},
	urldate = {2020-03-09},
	date = {2019},
	eprinttype = {arxiv},
	eprint = {1905.06290},
	keywords = {winograd},
	file = {arXiv Fulltext PDF:/home/tyler/Zotero/storage/VC7QLPVU/Kocijan et al. - 2019 - A Surprisingly Robust Trick for Winograd Schema Ch.pdf:application/pdf;arXiv.org Snapshot:/home/tyler/Zotero/storage/6CBWNYE3/1905.html:text/html}
}
