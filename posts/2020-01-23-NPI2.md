---
title: Words that Ever Play Well
subtitle: Part II on Negative Polarity Items
abstract: >
  In Part II we dive into the work of Anastasia Giannakidou, and come up with a
  new, more general, more powerful, and more correct licensing condition:
  the *variation approach*.

header-includes: >
  <style>
    blockquote {
      margin-bottom: 2.5rem;
      margin-left: 0;
      margin-right: 0;
    }

    blockquote > ol, blockquote > p {
      margin: 0 .2rem;
      font-size: 110%;
      background: #F1F1F1;
      border: 1px solid #E1E1E1;
      border-radius: 4px;
      padding: 1rem 1.5rem;
    }

    /* I'll use ~~ ~~ to mark bad sentences */
    del {
      color: red;
      text-decoration: none;
    }
  </style>
---

Review
======
[In Part I](./2020-01-19-NPI.html) we introduced *Negative Polarity Items*, a
special class of words that just don't seem to *ever* play right. We tried to
tackle the question **"in what contexts can we use NPIs?"**.

Ultimately we came up with (what I would call) a pretty compelling answer;
*downward entailment* is when something you know about $X$ tells you
something about a subset of $X$.

> (@dd) No dog can fly. $\vdash$ No talking dog can fly.
> (@du) A talking dog can fly. $\vdash$ A dog can fly.

Because "talking dogs" $\subseteq$ "dogs", we call (@dd) a *downward
entailment* and (@du) an *upward entailment*. And the magic is that **DEs
*licence* our troublesome NPIs**!

> No dog can *ever* fly.\
> ~~#A dog can *ever* fly.~~

In the end, however, we were left with a problematic sentence:

> Exactly 10 people have *ever* been to my house.

This sentence is **neither upward nor downward entailing.** In this post, we're
going to do something about that!

If you're reading this, I'm assuming Part I already got your attention. So, if
you'll permit me, I'm going to get a *little more technical* here. Time to
put on our big-kid-amateur-semanticist pants!

The Big Picture
===============
Before we get started, let's take a deep breath and ask ourselves, **"why does
this matter?"** It's fun, sure, but does it have a real impact on our ideas
of language?

Well-Formedness
---------------
To answer your question:

> (@col) Colorless green ideas sleep furiously.

Noam Chompsky said that, and I think he knows a thing or two! It was to
demonstrate (among other things) **there exists well-formed sentences that are
meaningless**. So what do I mean by well-formed?

> (@bcol) ~~#Furiously sleep ideas green colorless.~~

You're brain was able to "read" (@col). Were I to ask you "what sleeps?" you
could respond "colorless green ideas... whatever that means." Example (@bcol),
however, hardly deserves the title "sentence". It is not *grammatically
well-formed*. Let's look at some other examples:

Table: "Meaningful" here really means "semantically" (as opposed to
grammatically) well-formed.

Sentence                             Grammatic  Meaningful
-----------------------------------  ---------  -----------------------------
The talking dog can bark.            Yes        ~~No~~[^talkdog]
The talking dog bark can.            ~~No~~     ~~No~~[^almost]
The capital of Tyler is Vientiane.   Yes        ~~No~~[^capital]
The capital of Loas is Vientiane.    Yes        Yes
Fwogs can run.                       Yes(ish)   ~~No~~[^fwogs]
Fwog run home.                       ~~No~~     ~~No~~[^fwog]

[^talkdog]: "The talking dog can bark" is meaningless in a subtle kind of way.
  The problem is, it *presupposes* that there is a talking dog.  Because there
  is no such thing, the "subject" is an empty set.  "Colorless green things are
  bad" is more obviously wrong, but for the same reason --- there is no such
  thing. Interestingly, we are able to say what they *would* mean if there
  *were* such things.

[^almost]: One could argue "the dog bark can" is meaningful --- an English
  speaker could "get the gist", even though it's not a sentence. This is
  another topic for another day.

[^capital]: This kind of error is known as a *category faliure*. It's a kind of
  lexical anomaly that bind certain categories together. Here, the lexical
  category of people may not have capitals.

[^fwogs]: Although "fwogs" isn't a word, it fits the phonetic rules of English.
  Any speaker would be able to identify it as a plural noun.

[^fwog]: Again, the phonetics of English tell us that "fwog" must be a singular
  noun. Now there is a morphology problem --- "run" cannot be used with a
  singular subject.

NPI Errors
----------
What about ~~"#I found *anyone*"~~? Is that a failure of meaning, like (@col),
or of grammar, like (@bcol)? Qualitatively, it feels closer to a grammar
problem than a semantic problem.

And that is my (somewhat abstract) point --- we have been talking about NPI
errors as *semantic* in nature, having to do with what a sentence *means*,
rather than the structure of the sentence itself. **NPI errors bring into
question whether semantics play a roll in well-formedness.**

Situations Not Covered by D.E.
==============================
We've already found one example where downward entailment seemed to fail us. It
seems like the next step is to find some categories of

Modality
--------
Questions
---------
Disjunction
-----------
And More!
---------

A Solution
==========
Define *Veridacity*

Nonveridacity
-------------

Even "Stricter" NPIs &#8253;
---------------------------
*Antiveridacity*

D.E. $\subseteq$ Nonveridacity
-----------------------------

**Somethign about the role of syntax**

There's Always More To It
==========================
A section on how this is an ever-moving target. Should talk about
cross-linguistics here.

*Over / Under generation should maybe be touched on?*

Notes to Me
===========
With the introduction of cross-linguistic analysis, at is clear that NPIs
don't all work the same way.

If you ever talk, I'm going to hurt you.

'Order reversing closed under subsets.'

Postal 2000, Progovac 1994 show licencing as being "non-upward". Good middle
step.

Setction 3.2.1 Has examples of "not all NPIs in the same place."

Non-monotonic actually is slightly rare-er than in questions.

Section 4 is where we really get into the meat of things.

veridicality get's into the idea of presuposition.

"Zwarts 1995 further presents a proof that DE functions are a subset of the
nonveridical, hence nonveridicality is not in competition bur rather a
conservative extension of DE, that affords a broader empirical coverage and
strengthens the semantic theory of NPIs in just the right way. "

Sub-speciation of PI's. String/nonstrict NPIs, PPIs, FCI, Minimizers

Linebarger, negation, Page 19. Interesting note. I could use this to talk about
pragmatic levels of rules, and say "I am told this doesn't work. In any case
it's not what I'm writing about today."

Further sections of this paper I think go out of scope of what I want to cover.
I noticed that the book I just ordered is, in fact, out of date to this
research. I might want to investigate into more cutting edge texts.

I also was noticing that there is a readability trade-off for cross-linguistic
examples. *I* tend to gloss over them and they make me get tired reading. But
a single block of them at the end would **reall** not be all that interesting.
So... What does that leave me with? I have some sources I could use for Hungarian,
German, Dutch, Romance languages, and Japanese, which coveres most languages
my readers have.

This paper also get's into PPIs. I guess that goes into my different classes
of NPIs.

[^Giannakidou]: [Giannakidou, Anastasia.
  (2011)](https://www.researchgate.net/publication/255578263_Negative_and_positive_polarity_items_Variation_licensing_and_compositionality).
  Negative and positive polarity items: Variation, licensing, and
  compositionality. Semantics: An International Handbook of Natural
  Language Meaning.

